# LoRAå‚æ•°é«˜æ•ˆå¾®è°ƒ

### ä»€ä¹ˆæ˜¯å¾®è°ƒ

â€‹	éšç€æœºå™¨å­¦ä¹ çš„æœ€æ–°å‘å±•ï¼Œå¯¹æ¨¡å‹æ€§èƒ½çš„æœŸæœ›ä¹Ÿåœ¨å¢åŠ ï¼Œéœ€è¦æ›´å¤æ‚çš„æœºå™¨å­¦ä¹ æ–¹æ³•æ¥æ»¡è¶³å¯¹æ€§èƒ½çš„éœ€æ±‚ã€‚åœ¨æœºå™¨å­¦ä¹ çš„æ—©æœŸé˜¶æ®µï¼Œæ„å»ºä¸€ä¸ªæ¨¡å‹å¹¶åœ¨å•æ¬¡è®­ç»ƒä¸­è®­ç»ƒå®ƒæ˜¯å¯è¡Œçš„ã€‚

![image-20250530132358156](image/image-20250530132358156.png)

â€‹	è®­ç»ƒï¼Œåœ¨å…¶æœ€ç®€å•çš„æ„ä¹‰ä¸Šã€‚æ‚¨å°†ä¸€ä¸ªæœªç»è®­ç»ƒçš„æ¨¡å‹ï¼Œæä¾›ç»™å®ƒæ•°æ®ï¼Œå¹¶è·å¾—ä¸€ä¸ªé«˜æ€§èƒ½çš„æ¨¡å‹ã€‚

â€ƒâ€ƒå¯¹äºç®€å•é—®é¢˜æ¥è¯´ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ç§æµè¡Œçš„ç­–ç•¥ï¼Œä½†å¯¹äºæ›´å¤æ‚çš„é—®é¢˜ï¼Œå°†è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œå³â€œé¢„è®­ç»ƒâ€å’Œâ€œå¾®è°ƒâ€ï¼Œå¯èƒ½ä¼šå¾ˆæœ‰ç”¨ã€‚æ€»ä½“æ€è·¯æ˜¯åœ¨ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œåˆå§‹è®­ç»ƒï¼Œå¹¶åœ¨ä¸€ä¸ªå®šåˆ¶çš„æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚

<img src="image/image-20250530132701927.png" alt="image-20250530132701927" style="zoom:50%;" />

â€‹	è¿™ç§â€œé¢„è®­ç»ƒâ€ç„¶åâ€œå¾®è°ƒâ€çš„ç­–ç•¥å¯ä»¥è®©æ•°æ®ç§‘å­¦å®¶åˆ©ç”¨å¤šç§å½¢å¼çš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹æ¥å®Œæˆç‰¹å®šä»»åŠ¡ã€‚å› æ­¤ï¼Œé¢„è®­ç»ƒç„¶åå¾®è°ƒæ˜¯ä¸€ç§å¸¸è§ä¸”éå¸¸å¼ºå¤§çš„èŒƒä¾‹ã€‚

â€ƒâ€ƒæœ€åŸºæœ¬çš„å¾®è°ƒå½¢å¼ æ˜¯ä½¿ç”¨ä¸é¢„è®­ç»ƒæ¨¡å‹ç›¸åŒçš„è¿‡ç¨‹æ¥å¾®è°ƒæ–°æ•°æ®ä¸Šçš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡åœ¨å¤§é‡çš„é€šç”¨æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç„¶åä½¿ç”¨ç›¸åŒçš„è®­ç»ƒç­–ç•¥ï¼Œåœ¨æ›´å…·ä½“çš„æ•°æ®é›†ä¸Šå¾®è°ƒè¯¥æ¨¡å‹ã€‚

â€ƒâ€ƒ ä»¥ä¸Šç­–ç•¥ååˆ†æ˜‚è´µ ã€‚LLMsç»å¯¹åºå¤§ï¼Œéœ€è¦è¶³å¤Ÿçš„å†…å­˜æ¥å­˜å‚¨æ•´ä¸ªæ¨¡å‹ï¼Œä»¥åŠæ¨¡å‹ä¸­æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼ˆæ¢¯åº¦æ˜¯è®©æ¨¡å‹çŸ¥é“è°ƒæ•´å‚æ•°æ–¹å‘çš„ä¸œè¥¿ï¼‰ã€‚å‚æ•°å’Œæ¢¯åº¦éƒ½éœ€è¦å­˜åœ¨äºGPUä¸Šï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆè®­ç»ƒLLMséœ€è¦å¦‚æ­¤å¤šçš„GPUæ˜¾å­˜çš„ã€‚
![image-20250530132734650](image/image-20250530132734650.png)

### LoRAå¾®è°ƒ

LoRA åŸºäºå¤§æ¨¡å‹çš„å†…åœ¨ä½ç§©ç‰¹æ€§ï¼Œå¢åŠ æ—è·¯çŸ©é˜µæ¥æ¨¡æ‹Ÿå…¨å‚æ•°å¾®è°ƒï¼Œæ˜¯ç›®å‰æœ€é€šç”¨ã€æ•ˆæœæœ€å¥½çš„å¾®è°ƒæ–¹æ³•ä¹‹ä¸€ï¼Œè€Œä¸”èƒ½å’Œå…¶å®ƒå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•æœ‰æ•ˆç»“åˆã€‚åˆ©ç”¨è¯¥æ–¹æ³•å¯¹ 175B GPT-3 å¾®è°ƒï¼Œéœ€è¦è®­ç»ƒæ›´æ–°çš„å‚æ•°é‡å¯ä»¥å°åˆ°å…¨é‡å¾®è°ƒå‚æ•°é‡çš„ 0.01%ã€‚

<img src="image/image-20250530133335193.png" alt="image-20250530133335193" style="zoom:50%;" />

ä¸Šå›¾ä¸º LoRA çš„å®ç°åŸç†ï¼Œå…¶å®ç°æµç¨‹ä¸ºï¼š

â€‹	1ã€åœ¨åŸå§‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ—è¾¹å¢åŠ ä¸€ä¸ªæ—è·¯ï¼Œåšé™ç»´å†å‡ç»´çš„æ“ä½œæ¥æ¨¡æ‹Ÿå†…åœ¨ç§©ï¼›

â€‹	2ã€ç”¨éšæœºé«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ– Aï¼Œç”¨é›¶çŸ©é˜µåˆå§‹åŒ–Bï¼Œè®­ç»ƒæ—¶å›ºå®šé¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ï¼Œåªè®­ç»ƒçŸ©é˜µ A ä¸çŸ©é˜µ B ï¼›

â€‹	3ã€è®­ç»ƒå®Œæˆåï¼Œå°† B çŸ©é˜µä¸ A çŸ©é˜µç›¸ä¹˜ååˆå¹¶é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ä½œä¸ºå¾®è°ƒåçš„æ¨¡å‹å‚æ•°ã€‚

åœ¨æœ¬æ¡ˆä¾‹ä¸­ï¼Œå°†ä½¿ç”¨LoRAå¯¹Qwen2.5ç³»åˆ—çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨çš„æ•°æ®é›†æ˜¯shareAI-Llama3-DPO-zh-en-emojiï¼Œè¿™æ˜¯ä¸€ä¸ªemojiæ•°æ®é›†ï¼Œå¯ä»¥è®©æ¨¡å‹çš„å›ç­”æ›´åŠ ç”ŸåŠ¨æœ‰è¶£ï¼Œå¹¶ä¸”ä¼šæ·»åŠ ä¸€äº›emojiè¡¨æƒ…è®©å›ç­”çœ‹èµ·æ¥æ›´åŠ çœŸå®ã€‚

æœ¬æ¡ˆä¾‹ä½¿ç”¨çš„æ¨¡å‹ä¸ºQwen2.5-0.5Bï¼Œè‹¥è®¡ç®—èµ„æºå……è¶³çš„è¯ï¼Œå¯ä»¥å°è¯•æ›´å¤§ä¸€ç‚¹çš„æ¨¡å‹ã€‚

### å‡†å¤‡å·¥ä½œ

ä½œä¸ºå®éªŒçš„å‰æœŸå‡†å¤‡å·¥ä½œï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½æ•°æ®é›†ï¼Œæ¨¡å‹ï¼Œå¿…è¦çš„ä¾èµ–åº“ã€‚

modelscopeä½œä¸ºä¸€ä¸ªå¼€æ”¾å¹³å°ï¼Œæä¾›äº†å¾ˆå¤šæ•°æ®é›†ï¼Œæ¨¡å‹èµ„æºçš„ä¸‹è½½æ–¹å¼ï¼Œå¯¹å›½å†…ä¸‹è½½ä¹Ÿå¾ˆæ–¹ä¾¿ã€‚

é¦–å…ˆå®‰è£…modelscopeä¾èµ–åº“ï¼Œåç»­èµ„æºçš„ä¸‹è½½éƒ½è¦åœ¨è¿™ä¸ªä¸Šé¢è¿›è¡Œã€‚

```bash
pip install modelscope -i https://pypi.tuna.tsinghua.edu.cn/simple
```



#### ä¸‹è½½æ¨¡å‹

![image-20250530144435535](image/image-20250530144435535.png)

åˆ›å»ºä¸€ä¸ªæ¨¡å‹ç›®å½•ç”¨æ¥å­˜æ”¾æ¨¡å‹

```bash
mkdir Qwen2.5-0.5b-instruct
```

ä¸‹è½½å®Œæ•´æ¨¡å‹åº“ï¼š

```bash
modelscope download --model Qwen/Qwen2.5-0.5B-Instruct --local_dir Qwen2.5-0.5b-instruct
```



#### ä¸‹è½½æ•°æ®é›†

![image-20250530145805926](image/image-20250530145805926.png)

æ•°æ®é›†æ ¼å¼å¦‚ä¸‹ï¼š

```txt
{"question": "ä¸ºä»€ä¹ˆä¸­å›½è®¸å¤šåœ°åä»¥å¼ å®¶å‘½å  å¼ å®¶å£å¼ å®¶ç•Œ å¼ å®¶æ¸¯â€¦â€¦ï¼Ÿ", "answer_zh": "å“å‘€ï¼Œè€å…„ï¼Œä½ é—®åˆ°ç‚¹å­ä¸Šäº†ï¼ ğŸ˜„\n\nä½ çŸ¥é“å—ï¼Ÿåœ¨ä¸­å›½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ—æ–¹å’Œä¸œåŒ—åœ°åŒºï¼Œå§“å¼ çš„äººç‰¹åˆ«å¤šï¼æ®è¯´ï¼Œå¼ å§“æ˜¯ä¸­å›½ç¬¬äºŒå¤§å§“ï¼Œå å…¨å›½äººå£çš„è¿‘10%ï¼ ğŸ¤¯\n\né‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆå¾ˆå¤šåœ°åä»¥å¼ å®¶å‘½åå‘¢ï¼Ÿå…¶å®ï¼Œè¿™è·Ÿä¸­å›½çš„å†å²å’Œä¼ ç»Ÿæœ‰å…³ã€‚ ğŸ˜Š\n\nåœ¨å¤ä»£ï¼Œä¸­å›½çš„åœŸåœ°éƒ½æ˜¯åˆ†å°ç»™è´µæ—å’Œå®˜å‘˜çš„ï¼Œä»–ä»¬å°±ä¼šä»¥è‡ªå·±çš„å§“æ°å‘½åå½“åœ°ã€‚æ¯”å¦‚ï¼ŒæŸä¸ªå§“å¼ çš„å®˜å‘˜è¢«å°åˆ°æŸä¸ªåœ°æ–¹ï¼Œå°±ä¼šä»¥è‡ªå·±çš„å§“æ°å‘½åå½“åœ°ï¼Œå˜æˆâ€œå¼ å®¶å£â€ã€â€œå¼ å®¶ç•Œâ€ç­‰ç­‰ã€‚\n\nå¦å¤–ï¼Œåœ¨æ¸…æœå’Œæ°‘å›½æ—¶æœŸï¼Œä¸­å›½çš„ä¹¡æ‘ç¤¾ä¼šéå¸¸å‘è¾¾ï¼Œå§“å¼ çš„äººå¾ˆå¤šéƒ½æ˜¯å½“åœ°çš„å¯Œè±ªå’Œåœ°ä¸»ã€‚ä»–ä»¬å°±ä¼šä»¥è‡ªå·±çš„å§“æ°å‘½åå½“åœ°çš„æ‘åº„ã€é•‡ã€ç”šè‡³å¿åŸã€‚\n\nè¿˜æœ‰ä¸€ä¸ªåŸå› ï¼Œå°±æ˜¯å› ä¸ºâ€œå¼ â€å­—åœ¨ä¸­æ–‡ä¸­éå¸¸å¥½å¬ï¼Œå¾ˆå®¹æ˜“å¿µå’Œå†™ã€‚ ğŸ˜Š æ‰€ä»¥ï¼Œå¾ˆå¤šåœ°æ–¹éƒ½ä¼šé€‰æ‹©â€œå¼ â€å­—ä½œä¸ºåœ°åçš„ä¸€éƒ¨åˆ†ã€‚\n\næ€»ä¹‹ï¼Œä»¥å¼ å®¶å‘½åçš„åŸå› æœ‰å¾ˆå¤šï¼Œä½†ä¸»è¦æ˜¯å› ä¸ºå§“å¼ çš„äººåœ¨ä¸­å›½å†å²å’Œç¤¾ä¼šä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ ğŸ‘\n\nä½ ç°åœ¨çŸ¥é“äº†å—ï¼Ÿ ğŸ˜„" }

```

åˆ›å»ºä¸€ä¸ªç›®å½•å­˜å‚¨æ•°æ®é›†å†…å®¹ï¼š

```bash
mkdir emoji-datasets
```

ä¸‹è½½å®Œæ•´æ•°æ®é›†

```bash
modelscope download --dataset shareAI/shareAI-Llama3-DPO-zh-en-emoji --local_dir emoji-datasets
```



#### å®‰è£…ä¾èµ–åŒ…

```bash
pip install transformers torch peft datasets -i https://pypi.tuna.tsinghua.edu.cn/simple
```

è‡³æ­¤ï¼Œå‰æœŸçš„å‡†å¤‡å·¥ä½œå®Œæˆï¼



### LoRAå¾®è°ƒ

é¦–å…ˆå¯¼å…¥å¿…è¦ä¾èµ–åŒ…ï¼š

```Python

import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from datasets import Dataset
```

è®¾ç½®ä½ è‡ªå·±çš„æœ¬åœ°è·¯å¾„

```python
# è·¯å¾„è®¾ç½®
model_path = "./homeqwen2.5-0.5B-instruct"
dataset_path = "./emoji/2_dpo_ruozhiba.jsonl"
output_dir = "./qwen2.5-0.5B-lora-finetuned"

```

å¤„ç†æ•°æ®é›†

```python
def load_dataset(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line.strip())
            # æ„é€ è¾“å…¥-è¾“å‡ºå¯¹
            text = f"ç”¨æˆ·: {item['question']} åŠ©æ‰‹: {item['answer_zh']}"
            data.append({"text": text})
    return Dataset.from_list(data)



# æ•°æ®é›†é¢„å¤„ç†
def preprocess_function(examples):
    # åˆ†è¯å¹¶ç”Ÿæˆ input_ids å’Œ labels
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512,
        return_tensors="pt"
    )
    return {
        "input_ids": tokenized["input_ids"],
        "attention_mask": tokenized["attention_mask"],
        "labels": tokenized["input_ids"].clone()  # ç›‘ç£å¾®è°ƒä½¿ç”¨ç›¸åŒçš„ input_ids ä½œä¸º labels
    }

# åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†
dataset = load_dataset(dataset_path)
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=["text"])
```



é…ç½®LoRAå‚æ•°

```python
# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="eager"  # è§„é¿æ»‘åŠ¨çª—å£æ³¨æ„åŠ›è­¦å‘Š
)

# LoRA é…ç½®
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    bias="none",
    task_type="CAUSAL_LM"
)

# åº”ç”¨ LoRA
model = get_peft_model(model, lora_config)
```



å¼€å§‹è®­ç»ƒ

```python
# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    num_train_epochs=3,
    logging_steps=10,
    save_total_limit=2,
    remove_unused_columns=True,
)

# åˆå§‹åŒ– Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()

# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"æ¨¡å‹å¾®è°ƒå®Œæˆå¹¶ä¿å­˜è‡³ {output_dir}")
```

è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹

![image-20250530170114605](image/image-20250530170114605.png)

è®­ç»ƒå®Œæˆåï¼Œå¯¼å…¥LoRAæƒé‡è¿›è¡Œæ¨ç†ï¼š

```Python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model_path = "./qwen2.5-0.5B-instruct" #æ›¿æ¢ä¸ºä½ çš„å®é™…è·¯å¾„
lora_checkpoint_path = "./qwen2.5-0.5B-lora-finetuned"

tokenizer = AutoTokenizer.from_pretrained(lora_checkpoint_path)

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="eager"  
)


model = PeftModel.from_pretrained(base_model, lora_checkpoint_path, is_trainable=False)
model.eval()

# æ¨ç†å‡½æ•°
def generate_response(question, max_length=512):
    prompt = f"ç”¨æˆ·: {question} åŠ©æ‰‹: "
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=128)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    outputs = model.generate(
        **inputs,
        max_length=max_length,
        num_beams=4,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        no_repeat_ngram_size=2
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
   
    response = response.split("åŠ©æ‰‹: ")[-1].strip()
    return response

while (1):
    question=input("ç”¨æˆ·").strip()
    if question == "stop":
        break
    # question = "å¤©ä¸‹æ²¡æœ‰ä¸æ•£çš„ç­µå¸­ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç›¸èšçš„æ„ä¹‰åˆæ˜¯ä»€ä¹ˆï¼Ÿ"
    response = generate_response(question)
    print(f"é—®é¢˜: {question}")
    print(f"å›ç­”: {response}")
```

å¾®è°ƒå‰å¯¹è¯æ•ˆæœï¼š

![image-20250529171928003](image/image-20250529171928003.png)

å¾®è°ƒåæ•ˆæœï¼š

![image-20250529171644153](image/image-20250529171644153.png)