# å¾®è°ƒRoBERTaè¿›è¡Œæ–‡æœ¬åˆ†ç±»

æœ¬æ¬¡æˆ‘ä»¬å®æˆ˜çš„å†…å®¹æ˜¯å¾®è°ƒRoBERTaå¹¶å¯¹æ–°çš„æ•°æ®é›†è¿›è¡Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚

é¦–å…ˆè¿™é‡Œæˆ‘ä»¬ç®€å•è®²è§£ä¸€ä¸‹ä»€ä¹ˆæ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼š

è¿™é‡Œæœ‰ä¸€å¥è¯Sentence1ï¼šâ€œæˆ‘å–œæ¬¢ä½ ã€‚â€æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°åˆ¤æ–­å‡ºæ¥è¿™å¥è¯æ˜¯ç§¯æçš„ã€‚åŒç†ï¼Œå¯¹äºå¥å­Sentence2:â€œæˆ‘è®¨åŒä½ ã€‚â€æˆ‘ä»¬ä¹Ÿå¯ä»¥çŸ¥é“è¿™å¥è¯æ˜¯æ¶ˆæçš„ã€‚

åƒä¸Šé¢è¿™æ ·æŠŠå¥å­å½’åˆ°ä¸åŒç±»åˆ«é‡Œé¢çš„åšæ³•å°±æ˜¯æ–‡æœ¬åˆ†ç±»ã€‚

åœ¨æœ¬æ¬¡å®æˆ˜ä¸­ï¼Œæˆ‘ä»¬å°†ç”¨10kæ¡å¤–å–æ•°æ®å¯¹RoBERTaæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶æœ€ç»ˆç”¨è‡ªå·±æ–°ç”Ÿæˆçš„ä¸€ç³»åˆ—æ•°æ®åŸºäºè®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

åœ¨æ­£å¼å¼€å§‹å‰å…ˆå®‰è£…å¿…è¦çš„ä¾èµ–åº“ï¼š

```bash
!pip install modelscope transformers datasets torch scikit-learn 
'accelerate>=0.26.0' -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### æ•°æ®é›†ä¸‹è½½

#### ğŸ“„ æ•°æ®é›† waimai_10k

#### æ¥æºï¼š

- æ¥è‡ª Hugging Face ä¸Šçš„å…¬å¼€æ•°æ®é›†ï¼š`XiangPan/waimai_10k`

- åŒ…å«çº¦ 10,000 æ¡æ¥è‡ªç¾å›¢å¤–å–çš„ä¸­æ–‡ç”¨æˆ·è¯„è®ºå’Œæƒ…æ„Ÿæ ‡ç­¾

  æ ·ä¾‹æ ¼å¼ï¼š

  ```text
  review,label
  "è¿™ä¸ªå¤–å–çœŸçš„ä¸é”™ï¼Œé…é€ä¹Ÿå¾ˆå¿«ï¼",1
  "èœå¤ªéš¾åƒäº†ï¼Œæ€åº¦ä¹Ÿå·®ï¼Œä¸ä¼šå†ç‚¹äº†ï¼",0
  ```

huggingfaceä¸Šæä¾›äº†è¯¥æ•°æ®é›†çš„ä¸‹è½½æ–¹æ³•ï¼Œä½†å›½å†…ç›´è¿hfå®˜ç½‘é€Ÿåº¦å¾ˆæ…¢ï¼Œè¿™é‡Œæ¨èä½¿ç”¨é•œåƒç½‘ç«™ï¼šhttps://hf-mirror.com

```bash
!pip install -U huggingface_hub -i https://pypi.tuna.tsinghua.edu.cn/simple
!export HF_ENDPOINT=https://hf-mirror.com 
```

åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ç”¨äºä¿å­˜æ•°æ®é›†ï¼š

```bash
!mkdir waimai_10k
```

ç„¶åé€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥ä¸‹è½½æ•°æ®é›†

```bash
!huggingface-cli download --repo-type dataset --resume-download XiangPan/waimai_10k --local-dir waimai_10k
```



### ä¸‹è½½RoBERTaæ¨¡å‹

#### 1. ğŸ¤– RoBERTaï¼ˆrbt3ï¼‰

RoBERTa æ˜¯ Facebook åŸºäº BERT æ”¹è¿›çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç‰¹ç‚¹åŒ…æ‹¬ï¼š

- å»é™¤äº† NSP ä»»åŠ¡ï¼Œåªä¿ç•™ MLM
- æ›´é•¿çš„è®­ç»ƒæ—¶é—´å’Œæ›´å¤§æ•°æ®é‡
- æ›´å¼ºçš„æ€§èƒ½è¡¨ç°

ä½ ä½¿ç”¨çš„æ˜¯å“ˆå·¥å¤§æ¨å‡ºçš„ **ä¸­æ–‡ RoBERTa-wwm-extï¼ˆrbt3ï¼‰** ç‰ˆæœ¬ï¼Œé€‚ç”¨äºä¸­æ–‡ NLP ä»»åŠ¡ã€‚

å®ƒæ˜¯ä¸€ä¸ª `BertForSequenceClassification` ç±»å‹çš„æ¨¡å‹ï¼ˆBERT/RoBERTa æœ¬è´¨ä¸€æ ·ï¼ŒåŒºåˆ«åªæ˜¯é¢„è®­ç»ƒç»†èŠ‚ï¼‰ï¼Œç”¨äºäºŒåˆ†ç±»ã€‚

modelscopeæä¾›äº†è¿™ä¸ªæ¨¡å‹çš„ä¸‹è½½åœ°å€

æ¨¡å‹çš„ä¸‹è½½è·¯å¾„ä¸ºï¼š

https://www.modelscope.cn/models/dienstag/rbt3/files

é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ¨¡å‹ç›®å½•ç”¨æ¥å­˜æ”¾æ¨¡å‹æ–‡ä»¶ï¼š

```bash
!mkdir rbt3
```

ä¸‹è½½æ¨¡å‹ï¼š

```bash
modelscope download --model dienstag/rbt3 --local_dir rbt3
```

### åŠ è½½æ•°æ®é›†

##### é¦–å…ˆåŠ è½½ç¬¬ä¸‰æ–¹åº“ä»¥åŠåˆšä¸‹è½½å¥½çš„æ•°æ®é›†

```Python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.utils.data import random_split

##åŠ è½½æ•°æ®

import pandas as pd
data = pd.read_csv('waimai_10k/waimai_10k.csv')
data.head()
print(f"Total number of samples: {len(data)}")
data = data.dropna()
print(f"Total number of samples after removing NaN: {len(data)}")

```

![image-20250417085034023](image/image-20250417085034023.png)

æ•´ä¸ªæ•°æ®é›†æ‹¥æœ‰11987æ¡æ•°æ®

##### åˆ›å»ºDatasetï¼Œè¿›è¡Œå®ä¾‹åŒ–ï¼Œæ–¹ä¾¿åç»­çš„è®­ç»ƒ

ç»§æ‰¿Pytorchçš„Datasetç±»å¹¶åˆ›å»ºè‡ªå·±çš„MyDatasetç±»ï¼Œäºæ­¤åŒæ—¶æˆ‘ä»¬è¿˜è¦æ”¹å†™ä¸¤ä¸ªæ–¹æ³•ï¼Œä¸€ä¸ªæ˜¯__len__ï¼Œå¦ä¸€ä¸ªæ˜¯__getitem__ã€‚

```python
class MyDataset(Dataset):
    def __init__(self, data):
        super().__init__()
        self.data = data.dropna()

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        return self.data.iloc[index]['review'], self.data.iloc[index]['label']

##å°†MyDatasetè¿›è¡Œå®ä¾‹åŒ–
dataset = MyDataset(data)
print(dataset[0])
print(len(dataset))
```

![image-20250417085259207](image/image-20250417085259207.png)

åœ¨è¿™æ¡æ•°æ®é›†ä¸­ï¼Œâ€œå¾ˆå¿«ï¼Œå¥½åƒï¼Œå‘³é“è¶³ï¼Œé‡å¤§â€ï¼Œä¸ºç”¨æˆ·çš„è¯„ä»·ï¼Œâ€œ1â€ åˆ™è¡¨ç¤ºè¿™æ¡è¯„ä»·æ˜¯ç§¯æçš„ï¼ˆ0æ˜¯æ¶ˆæçš„ï¼‰

##### åˆ’åˆ†æ•°æ®é›†

è¿™é‡Œæˆ‘ä»¬æŒ‰ç…§0.8å’Œ0.2çš„æ¯”ä¾‹å¯¹æ•°æ®é›†è¿›è¡Œåˆ’åˆ†ï¼Œå°†80%çš„æ•°æ®ä½œä¸ºè®­ç»ƒé›†ï¼Œ20%çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†ã€‚

```bash
trainset, testset = random_split(dataset, [0.8, 0.2])
print(f"Total number of samples: {len(dataset)}")
print(f"Number of training samples: {len(trainset)}")
print(f"Number of test samples: {len(testset)}")
print(trainset[0])
```

![image-20250417090009394](image/image-20250417090009394.png)

##### åˆ›å»ºDataLoader

è¿™é‡Œæˆ‘ä»¬æ ¹æ®åˆšæ‰åˆ›å»ºçš„Datasetå»åˆ›å»ºDataLoaderï¼Œå…¶ä¸­è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„Batch_sizeéƒ½è®¾ç½®ä¸º64ã€‚ä½†åœ¨è¿™é‡Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªcollate_funcå‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯å¯¹æˆ‘ä»¬æ•°æ®é›†çš„æ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ“ä½œï¼Œå°†å…¶tokenizeåŒ–ä¹‹åå˜æˆtensorå‘é‡ã€‚

```python
tokenizer = AutoTokenizer.from_pretrained("rbt3")

def collate_func(batch):
    texts, labels = [], []
    for item in batch:
        texts.append(item[0])
        labels.append(item[1])
    inputs = tokenizer(texts, padding="max_length", truncation=True, max_length=512, return_tensors="pt")
    inputs["labels"] = torch.tensor(labels)
    return inputs

train_dataloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate_func)
test_dataloader = DataLoader(testset, batch_size=32, shuffle=False, collate_fn=collate_func)

print(next(iter(train_dataloader)))
```

![image-20250417090131185](image/image-20250417090131185.png)

##### åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨

è¿™é‡Œæˆ‘ä»¬åˆ›å»ºçš„ä¼˜åŒ–å™¨ä¸ºAdamï¼Œæ¨¡å‹æ˜¯ä»hugging faceä¸Šä¸‹è½½åŠ è½½é¢„è®­ç»ƒå¥½çš„RoBERTaæ¨¡å‹ã€‚

```python
model=AutoModelForSequenceClassification.from_pretrained("rbt3")
optimizer=optim.AdamW(model.parameters(),lr=1e-5)

if torch.cuda.is_available():
    model.cuda()
model.eval()

# æ¨ç†
sentence1 = "è¿™å®¶é¥­åº—çœŸè´´å¿ƒï¼ŒçŸ¥é“æˆ‘åƒä¸é¥±è¿˜ç‰¹åœ°åœ¨é‡Œé¢æ”¾èŸ‘è‚"
id2_label = {0: "negative", 1: "positive"}

with torch.inference_mode():
    inputs = tokenizer(sentence1, return_tensors="pt", padding=True, truncation=True)
    inputs = {k: v.cuda() for k, v in inputs.items()}
    logits = model(**inputs).logits
    softmax_ = torch.nn.functional.softmax(logits, dim=1)
    predicted_class = torch.argmax(logits, dim=-1)
    print("åŸæ¨¡å‹ï¼š")
    print(f"Sentence: {sentence1}")
    print(f"Predicted class: {id2_label.get(predicted_class.item())}")
    print(f"Confidence: {softmax_[0][predicted_class.item()]:.4f}")
```

ä»ç»“æœå¯ä»¥çœ‹å‡ºï¼Œæœªç»è¿‡è®­ç»ƒå¾®è°ƒçš„RoBERTaæ¨¡å‹ï¼Œé¢„æµ‹ç½®ä¿¡åº¦åªæœ‰0.62

![image-20250417090833859](image/image-20250417090833859.png)

##### è®­ç»ƒå’ŒéªŒè¯

éªŒè¯ä»£ç ï¼š

```python
def evaluate():
    model.eval()
    correct = 0
    with torch.inference_mode():
        for batch in test_dataloader:
            if torch.cuda.is_available():
                batch = {k: v.cuda() for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1)
            correct += (predictions == batch['labels']).sum().item()
    return correct / len(test_dataloader.dataset) * 100  ## è¿”å›å‡†ç¡®ç‡
```

è®­ç»ƒä»£ç ï¼š

```python
def train(epoch=3, log_step=50):
    global_step = 0
    for ep in range(epoch):
        model.train()
        for batch in train_dataloader:
            if torch.cuda.is_available():
                batch = {k: v.cuda() for k, v in batch.items()}
            optimizer.zero_grad()
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            if (global_step+1) % log_step == 0:
                print(f"Epoch: {ep+1}, global_step: {global_step+1}, Loss: {loss.item()}")
            global_step += 1

        acc = evaluate()
        print(f"Epoch: {ep+1}, Accuracy: {acc:.2f}%")
    # ä¿å­˜æ¨¡å‹æƒé‡
    save_path = "rbt3_finetuned"
    model.save_pretrained(save_path)
    print(f"Model weights saved to {save_path}")

train() #å¼€å§‹è®­ç»ƒ
```

åœ¨è¿™é‡Œï¼Œè®¾ç½®çš„è®­ç»ƒè½®æ•°æ˜¯3è½®ï¼Œlogæ¢¯åº¦ä¸º50ï¼Œå­¦ä¹ ç‡ä¸º1e-5ï¼Œå¤§å®¶å¯ä»¥é€‚å½“è°ƒæ•´è®­ç»ƒå‚æ•°ï¼Œä»è€Œä¼˜åŒ–è®­ç»ƒæ•ˆç‡é™ä½Lossã€‚

è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹ï¼š

![image-20250417091915596](image/image-20250417091915596.png)

##### æ¨¡å‹æ¨ç†

```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch

save_path = "rbt3_finetuned"

# åŠ è½½ä½ è®­ç»ƒå¥½çš„æ¨¡å‹
save_path = "rbt3_finetuned"
model = BertForSequenceClassification.from_pretrained(save_path)
model.cuda()
model.eval()

# æ¨ç†
sentence1 = "è¿™å®¶é¥­åº—çœŸè´´å¿ƒï¼ŒçŸ¥é“æˆ‘åƒä¸é¥±è¿˜ç‰¹åœ°åœ¨é‡Œé¢æ”¾èŸ‘è‚"
id2_label = {0: "negative", 1: "positive"}

with torch.inference_mode():
    inputs = tokenizer(sentence1, return_tensors="pt", padding=True, truncation=True)
    inputs = {k: v.cuda() for k, v in inputs.items()}
    logits = model(**inputs).logits
    softmax_ = torch.nn.functional.softmax(logits, dim=1)
    predicted_class = torch.argmax(logits, dim=-1)

    print("è®­ç»ƒå¥½çš„æ¨¡å‹ï¼š")
    print(f"Sentence: {sentence1}")
    print(f"Predicted class: {id2_label.get(predicted_class.item())}")
    print(f"Confidence: {softmax_[0][predicted_class.item()]:.4f}")
```

è®­ç»ƒç»“æœå¦‚ä¸‹ï¼š

![image-20250417092504480](image/image-20250417092504480.png)

ä»ç»“æœå¯ä»¥çœ‹å‡ºï¼Œæ¨¡å‹æ¨ç†çš„ç½®ä¿¡åº¦ä¸º0.82ï¼Œç›¸è¾ƒäºè®­ç»ƒå‰çš„0.62ï¼Œè®­ç»ƒåçš„æ¨¡å‹ç½®ä¿¡åº¦æœ‰äº†è¾ƒå¤§çš„æå‡ã€‚

