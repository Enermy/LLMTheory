# LLMTheory
这是一个大模型的实战项目，介绍一些大模型方面的知识，包括但不限于BERT，Transformer，RoBERT，Attention机制等。

Class1：
介绍一下注意力机制的原理与QKV的计算，并附上代码实现的详细过程

https://github.com/Enermy/LLMTheory/tree/main/Class1

Class2：
动手实现Transformer的代码，并使用该模型来实现一个简单的机器翻译任务

https://github.com/Enermy/LLMTheory/tree/main/Class2

Class3:
通过 NLP 中最常见的文本分类任务来学习如何在自己的数据集上利用迁移学习（transfer learning）微调一个预训练的 Transformer 模型—— DistilBERT。DistilBERT 是 BERT 的一个衍生版本，它的优点在它的性能与 BERT 相当，但是体积更小、更高效。所以我们可以在几分钟内训练一个文本分类器。

https://github.com/Enermy/LLMTheory/tree/main/Class3

Class4:
微调BERT进行文本分类，Bert是基于Transformer的编码器和海量文本训练出的预训练模型，他通过学习完型填空和上下句预测的任务来获得类似人类一样对通用语义的理解能力。

https://github.com/Enermy/LLMTheory/tree/main/Class4

Class5：
微调RoBERTa进行文本分类，用10k条外卖数据对RoBERTa模型进行微调，并最终用自己新生成的一系列数据基于训练好的模型进行推理。

https://github.com/Enermy/LLMTheory/tree/main/Class5

Class6：
预训练GPT2模型生成文本

https://github.com/Enermy/LLMTheory/tree/main/Class6

Class7：
微调GPT2模型生成目标风格文本

https://github.com/Enermy/LLMTheory/tree/main/Class7

Class8:
文本摘要提取,本文将详细介绍Python中的一种主流的文本摘要方法-T5模型，并结合实际案例进行分析。

https://github.com/Enermy/LLMTheory/tree/main/Class8

Class9:
微调BERT进行情感分析任务

https://github.com/Enermy/LLMTheory/tree/main/Class9

Class10：
微调BERT进行虚假新闻检测任务

https://github.com/Enermy/LLMTheory/tree/main/Class10

Class11：
使用BERT-base-Chinese模型进行中文语义角色标注任务

https://github.com/Enermy/LLMTheory/tree/main/Class11

Class12：
建立本地知识库，搭建一个简易的问答机器人

https://github.com/Enermy/LLMTheory/tree/main/Class12

Class13：
LoRA参数高效微调

https://github.com/Enermy/LLMTheory/tree/main/Class13

